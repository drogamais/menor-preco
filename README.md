# ğŸ›ï¸ MP Feeder (v1.30) ğŸ›’

Algoritmo em Python para a captaÃ§Ã£o de notas fiscais da plataforma Menor PreÃ§o (Nota ParanÃ¡) e inserÃ§Ã£o em um banco de dados MariaDB.

O script foi desenvolvido para coletar dados de preÃ§os de concorrentes com base em uma lista de produtos (GTINs) e geolocalizaÃ§Ãµes (Geohashs) prÃ©-definidas.

## ğŸ§­ SumÃ¡rio

* [Principais Funcionalidades](#-principais-funcionalidades)
* [Como Usar](#-como-usar)
* [Fluxo de ExecuÃ§Ã£o](#-fluxo-de-execuÃ§Ã£o)
* [Estrutura do Projeto](#-estrutura-do-projeto)

## ğŸ¯ Resumo do Projeto

Este Ã© um pipeline de ETL robusto e tolerante a falhas projetado para:

*   Coletar dados de preÃ§os da API do Menor PreÃ§o (Nota ParanÃ¡).

*   Enriquecer os dados com geocodificaÃ§Ã£o de lojas (Google API) e notificaÃ§Ãµes (Telegram).

*   Carregar os dados em um banco MariaDB, com lÃ³gica de recuperaÃ§Ã£o automÃ¡tica em caso de falha.

## âœ¨ Principais Funcionalidades

Este projeto Ã© um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) completo e resiliente.

* ğŸ§  **AtualizaÃ§Ã£o Inteligente de Produtos:** Periodicamente (a cada 30+ dias), o script reconstrÃ³i a lista de 1000 produtos-alvo (`bronze_menorPreco_produtos`). Ele cruza os 2000 produtos mais vendidos por *valor* e *quantidade* da `bronze_plugpharma_vendas` e, em seguida, busca o **GTIN principal** (`codigo_principal = 1`) para cada um na `bronze_plugpharma_produtos`.

* ğŸ”„ **Coleta Rotativa (Batch):** O script nÃ£o consulta os 1000 produtos de uma vez. Ele divide a lista em lotes de 100 GTINs e processa um lote por execuÃ§Ã£o, continuando de onde parou na execuÃ§Ã£o anterior (lÃ³gica gerenciada pelo `ultimo_indice.txt` e `pegar_ultimo_gtin`).

* ğŸ£ **Coleta Ampla de Dados:** Utiliza os GTINs do lote como "isca" na API do Menor PreÃ§o. No entanto, ele salva *todos* os produtos que a API retorna na nota fiscal, nÃ£o apenas o produto-isca. Isso enriquece a tabela `bronze_menorPreco_notas` com uma vasta gama de produtos concorrentes.

* ğŸ—ºï¸ **GeocodificaÃ§Ã£o de Novas Lojas:** Ao encontrar uma loja (`id_loja`) nÃ£o cadastrada na `bronze_menorPreco_lojas`, o script utiliza a API do Google Geocoding para buscar suas coordenadas de latitude e longitude antes de salvÃ¡-la.

* ğŸ›¡ï¸ **TolerÃ¢ncia a Falhas (Banco de Dados):** Se a inserÃ§Ã£o final no banco de dados falhar (ex: perda de conexÃ£o), o `handle_execution_error` Ã© acionado. Ele salva *todos* os dados coletados (notas e lojas) em arquivos `.csv` locais (`notas_parciais.csv`, `lojas_parciais.csv`).

* ğŸ” **RecuperaÃ§Ã£o AutomÃ¡tica:** Na prÃ³xima execuÃ§Ã£o, o `main.py` detecta esses arquivos `.csv`. Ele primeiro executa o `run_recovery_flow`, que carrega os dados desses CSVs no banco de dados e depois os apaga, garantindo que nenhum dado seja perdido antes de iniciar uma nova coleta.

* ğŸ”” **Monitoramento e NotificaÃ§Ãµes:** Envia mensagens de sucesso ou erro para um chat do Telegram, permitindo o monitoramento remoto da execuÃ§Ã£o.

---

## ğŸš€ Como Usar

### 1. ğŸ“‹ PrÃ©-requisitos

Garanta que vocÃª tenha um banco de dados MariaDB acessÃ­vel. O script espera se conectar a um banco chamado `dbDrogamais`.

VocÃª precisarÃ¡ das seguintes tabelas (fontes e destino):
* `bronze_plugpharma_vendas` (para anÃ¡lise de vendas)
* `bronze_plugpharma_produtos` (para buscar GTINs principais)
* `bronze_cidades` (para buscar geohashs)
* `dbSults.tb_report_auditoria_embedded` (para filtrar geohashs)
* `bronze_menorPreco_produtos` (destino da lista de 1000 produtos)
* `bronze_menorPreco_notas` (destino dos dados brutos da API)
* `bronze_menorPreco_lojas` (destino das lojas concorrentes)

### 2. ğŸ’» InstalaÃ§Ã£o

Clone o repositÃ³rio e instale as dependÃªncias do Python:

```bash
pip install -r requirements.txt
```
---------------
### 3. ğŸ”‘ ConfiguraÃ§Ã£o

O script usa um arquivo config.py para armazenar suas chaves e senhas. Este arquivo Ã© ignorado pelo Git.

* 1. Copie o arquivo de exemplo:
```bash
copy config.py.example config.py
```

* 2. Abra o config.py e preencha as variÃ¡veis com suas credenciais:

`DB_CONFIG`: DicionÃ¡rio com user, password, host e port do seu MariaDB.

`GOOGLE_API_KEY`: Sua chave da API do Google Cloud (para o Geocoding).

`TELEGRAM_TOKEN`: O token do seu Bot do Telegram.

`TELEGRAM_CHAT_ID`: O ID do chat para onde as notificaÃ§Ãµes serÃ£o enviadas.

----------------------

### 4. â–¶ï¸ ExecuÃ§Ã£o

Uma vez configurado, basta executar o `main.py`:

```bash
python main.py
```

O script cuidarÃ¡ do resto, seja iniciando uma nova coleta ou recuperando dados de uma execuÃ§Ã£o anterior com falha.

## ğŸ“Š Fluxo de ExecuÃ§Ã£o

### 1. main.py Ã© iniciado.

### 2. Verifica Falha Anterior: O script procura pelo arquivo notas_parciais.csv.

### 3. Fluxo de RecuperaÃ§Ã£o (Se .csv existe):

    flow.run_recovery_flow Ã© chamado.

    Os dados dos arquivos .csv sÃ£o lidos e inseridos no banco de dados.

    Os arquivos .csv sÃ£o removidos apÃ³s o sucesso da carga.

### 4. Fluxo Normal (Se .csv nÃ£o existe):

    ow.run_normal_flow Ã© chamado.

*   **[E] ExtraÃ§Ã£o:**

    (Opcional) Atualiza a lista de 1000 produtos-alvo se tiver > 30 dias.

    Seleciona o lote de 100 GTINs do dia.

    Gera a lista de consultas (Geohash x GTIN).

* **[T] TransformaÃ§Ã£o (Coleta):**

    api_services.buscar_notas coleta os dados da API do Menor PreÃ§o.

    Retorna os DataFrames Notas_geral e Lojas_SC_geral para o main.py.

* **[L] Carga:**

    main.py recebe os DataFrames.

    (Opcional) api_services.buscar_lat_lon_lojas_sc enriquece Lojas_SC_geral com Lat/Lon do Google.

    db_manager.inserir_lojas_sc e db_manager.inserir_notas carregam os dados no MariaDB.

### 5. FinalizaÃ§Ã£o

    Sucesso: handle_success limpa o ultimo_indice.txt e envia notificaÃ§Ã£o de sucesso via Telegram.

    Falha (Ex: DB Offline): handle_execution_error Ã© chamado, save_partial_data cria os arquivos .csv para a prÃ³xima execuÃ§Ã£o (Passo 2) e envia notificaÃ§Ã£o de erro.


## ğŸ“‚ Estrutura do Projeto

*   `main.py`: ğŸš¦ Ponto de entrada. Orquestra os fluxos (normal vs. recuperaÃ§Ã£o) e a etapa de Carga (Load).

*   `flow.py`: ğŸƒâ€â™‚ï¸ ContÃ©m a lÃ³gica de negÃ³cio principal para run_normal_flow (ExtraÃ§Ã£o e TransformaÃ§Ã£o) e run_recovery_flow (Carga de CSVs).

*   `db_manager.py`: ğŸ—ƒï¸ Abstrai toda a comunicaÃ§Ã£o com o banco de dados MariaDB. ContÃ©m todas as queries SQL (SELECTs e INSERTs).

*   `api_services.py`: â˜ï¸ Gerencia todas as chamadas para APIs externas (Nota ParanÃ¡, Google Geocoding e Telegram).

*   `etl_utils.py`: ğŸ› ï¸ FunÃ§Ãµes auxiliares de transformaÃ§Ã£o de dados (lÃ³gica de Pandas), gerenciamento de estado (leitura/escrita do ultimo_indice.txt) e configuraÃ§Ã£o de logging.

*   `error_handler.py`: ğŸš¨ FunÃ§Ãµes centralizadas para lidar com exceÃ§Ãµes, salvar dados parciais em CSV e notificar falhas.

*   `config.py` (e `.example`): ğŸ”’ Armazena as credenciais e chaves de API.

*   `requirements.txt`: ğŸ“¦ Lista de pacotes Python necessÃ¡rios.

*   `.gitignore`: ğŸ™ˆ Define os arquivos que nÃ£o devem ser versionados (logs, config.py, arquivos .csv, etc.).